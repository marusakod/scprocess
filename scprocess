#!/usr/bin/env python3

# PATH="${PATH:+${PATH}:}/home/macnairw/packages/scProcess/scprocess.py"

# example call:
# scprocess /home/macnairw/packages/scProcess/configs/config-myrf_mice.yaml

import argparse
import os
import yaml
import pandas as pd
import re
import glob
import subprocess


# find fastq files for a sample
def find_fastq_files(fastqs_dir, sample, read):
  # get all files
  all_fs    = glob.glob(f"{fastqs_dir}/*{sample}*")

  # get all reads
  re_R1     = re.compile('.*_R1.*\\.fastq(\\.gz)?$')
  R1_fs     = [ f for f in all_fs if re_R1.match(f) ]
  R1_fs     = sorted(R1_fs)
  re_R2     = re.compile('.*_R2.*\\.fastq(\\.gz)?$')
  R2_fs     = [ f for f in all_fs if re_R2.match(f) ]
  R2_fs     = sorted(R2_fs)

  # check they match
  R1_chk    = [ re.sub("_R1", "", f) for f in R1_fs ]
  R2_chk    = [ re.sub("_R2", "", f) for f in R2_fs ]
  assert R1_chk == R2_chk, "R1 and R2 fastq files do not match for " + sample

  if read == "R1":
    sel_fs = R1_fs
  elif read == "R2":
    sel_fs = R2_fs

  return sel_fs


# function to exclude samples without valid fastq files
def exclude_samples_without_fastq_files(FASTQS_DIR, SAMPLES, HTO = False):
  # get parameters

  # get fastq files for each sample
  chk_samples   = []
  for sample in SAMPLES:
    R1_fs = find_fastq_files(FASTQS_DIR, sample, "R1")
    if len(R1_fs) > 0:
      chk_samples.append(sample)
    else:
      if HTO:
        print(f"WARNING: no hto fastq files found for sample {sample}; excluded")
      else:
        print(f"WARNING: no fastq files found for sample {sample}; excluded")

  return chk_samples


def run_scprocess(sc_dir, configfile, snakefile, rule, extraargs):
  print('doing some checks on inputs')
  
  # check that SCPROCESS_DATA_DIR exists
  scprocess_data_dir = os.getenv('SCPROCESS_DATA_DIR')

  if not scprocess_data_dir:
    raise ValueError('SCPROCESS_DATA_DIR is not defined in .bashrc')
  
  assert os.path.isdir(scprocess_data_dir), \
   "SCPROCESS_DATA_DIR is not a directory"
  
  # check that spcrocess_data_dir has some files
  scsetup_dirs = ['cellranger_ref', 'gmt_pathways', 'marker_genes', 'xgboost', 'alevin_fry_home']
  scsetup_fulL_dirs = [os.path.join(scprocess_data_dir, d) for d in scsetup_dirs]
  scsetup_csv = os.path.join(scprocess_data_dir,'setup_parameters.csv')

  assert all([os.path.isdir(d) for d in scsetup_fulL_dirs]), \
   f"Some directories in {scprocess_data_dir} are missing; consider (re)running scsetup"
  
  assert os.path.isfile(scsetup_csv), \
   f"{scsetup_csv} is missing; consider (re)running scsetup"
  
  # check if cluster profile is defined
  setup_configfile = os.path.join(scprocess_data_dir, '.scprocess_setup.yaml')
  assert os.path.exists(setup_configfile), \
    f".scprocess_setup.yaml does not exist in {scprocess_data_dir}"

  with open(setup_configfile, "r") as stream:
    setup_config      = yaml.safe_load(stream)

  if ('profile' in setup_config) and setup_config['profile'] is not None:
    profile = setup_config['profile']
    
    # check if profile exists
    profile_dir = os.path.join(sc_dir, 'profiles', profile)
    profile_f   = os.path.join(profile_dir, 'config.yaml')

    assert os.path.isfile(profile_f), \
      f"cluster configuration file {profile_f} does not exist"
    
    # add snakemake flag
    extraargs = extraargs + ' --workflow-profile ' + profile_dir


  # check configfile exists
  assert os.path.exists(configfile), \
    f"Config file {configfile} does not exist"
  
  # open configfile
  with open(configfile, "r") as stream:
    config      = yaml.safe_load(stream)

  # check that proj_dir exists and is a directory
  assert "proj_dir" in config, "proj_dir not in config file"
  assert os.path.isdir(config["proj_dir"]), \
    f"proj_dir {config['proj_dir']} is not a directory"
  proj_dir    = config["proj_dir"] 
  
  # check that proj_dir is a workflowr directory 
  wflowr_fs_ls = ['_workflowr.yml', '.gitignore', '.Rprofile', '.gitattributes',
            'analysis/_site.yml', 'analysis/about.Rmd', 'analysis/index.Rmd', 'analysis/license.Rmd', 'public/.nojekyll']
  
  wflowr_fs_ful_ls = [os.path.join(proj_dir, f) for f in wflowr_fs_ls]
  assert all([os.path.isfile(f) for f in wflowr_fs_ful_ls]), \
    f"proj_dir {config['proj_dir']} is not a workflowr project; you can create a workflowr project using newproj"
  
  # check if samples are multiplexed using htos
  sample_var = "sample_id" # id to use for matching with fastq files
  demux_type = None
  hto_fastq_dir = None

  if ('multiplexing' in config) and (config['multiplexing'] is not None):
    assert 'demux_type' in config['multiplexing']
    demux_type = config['multiplexing']['demux_type']
    assert demux_type in ["af", "custom"], \
      "Invalid value of 'demux_type' parameter. Must be either 'af' or 'custom'"

  # check that sample metadata exists, has sample_id present, no spaces in column names 
  assert "sample_metadata" in config, "sample_metadata not specified in config file"
  sample_metadata = config['sample_metadata']
  if not os.path.isabs(sample_metadata):
    sample_metadata = os.path.join(proj_dir, sample_metadata)
    
  assert os.path.isfile(sample_metadata), \
     f"sample metadata {sample_metadata} is not a valid file"
  
  # load up sample metadata
  sample_df   = pd.read_csv(sample_metadata)
  assert "sample_id" in sample_df.columns, \
    "sample_id not present in sample metadata"
  
  if demux_type is not None:
    sample_var = "pool_id"
    assert "pool_id" in sample_df.columns, \
     "pool_id not present in sample metadata"
    if demux_type == 'af':
     assert "hto_id" in sample_df.columns, \
      "hto_id not present in sample metadata"
     # check that hto fastq path is present
     assert "fastq_dir" in config['multiplexing'], \
      "fastq_dir missing from 'multiplexing' section in the config file"
     hto_fastq_dir = config['multiplexing']['fastq_dir'] 
     if not os.path.isabs(hto_fastq_dir):
       hto_fastq_dir = os.path.join(proj_dir, hto_fastq_dir)
     assert os.path.isdir(hto_fastq_dir), \
       f"{hto_fastq_dir} is not a valid directory"
  


  assert all(sample_df[ "sample_id" ].value_counts() == 1), \
    "sample_id values in metadata csv not unique"
  
  assert not any(' ' in col for col in sample_df.columns), \
    "some column names in metadata csv contain spaces."

  # check whether metadata_vars are present in sample metadata
  if "metadata_vars" in config:
    assert isinstance(config["metadata_vars"], list), \
      "metadata_vars must be a list"
    for var in config["metadata_vars"]:
      assert var in sample_df.columns, \
        f"metadata_var {var} not present in sample metadata"

  # check fastq directory exists
  assert "fastq_dir" in config, "fastq_dir not in config file"
  fastq_dir = config['fastq_dir']
  if not os.path.isabs(fastq_dir):
    fastq_dir = os.path.join(proj_dir, fastq_dir)
  assert os.path.isdir(fastq_dir), \
    f"{fastq_dir} if not a valid directory"
  
  
  samples_ls = sample_df[sample_var].tolist()
  # check that samples don't have '_R1' or '_R2' in their names 
  assert all('_R1' not in s and '_R2' not in s for s in samples_ls), \
    f"One or more {sample_var} values contain '_R1' or '_R2'. Please ensure all elements exclude these substrings"
  
  # check that some fastq files exist
  samples_ls  = list(dict.fromkeys(samples_ls))
  samples_ls  = exclude_samples_without_fastq_files(fastq_dir, samples_ls, HTO=False)

  if demux_type == 'af':
   samples_ls  = exclude_samples_without_fastq_files(hto_fastq_dir, samples_ls , HTO=True)

  assert len(samples_ls) > 0, \
    "No fastq files found in fastq_dir"

  # print which files will be processed
  print(f"Processing {len(samples_ls)} samples:")
  print('  ' + ', '.join(samples_ls))
  print('bash '+ f'{sc_dir}/scprocess.sh'+ ' ' +snakefile+ ' '+ os.path.abspath(configfile) + ' '+ proj_dir+ ' '+ rule+ ' '+ extraargs)

  # start snakemake
  subprocess.call(['bash', f'{sc_dir}/scprocess.sh', snakefile, os.path.abspath(configfile), proj_dir, rule, extraargs])




if __name__ == '__main__':
  # define scprocess directory
  sc_dir    = os.path.dirname(os.path.realpath(__file__))
  # sc_dir    = "/home/macnairw/packages/scProcess/"

  # define arguments
  parser    = argparse.ArgumentParser(description = 'snakemake workflows for processing single cell RNAseq data.')
  parser.add_argument("configfile", type = str, 
    help = "Required. YAML file specifying what you want to run, and any non-default parameters")
  parser.add_argument("-r", "--rule", type = str, default = "all",
    choices = [
        "all", "simpleaf", "ambient", "qc", "integration", 
        "marker_genes", "label_celltypes", "metacells", "pb_empties", "zoom"
        ],
    help = '''
       Leave empty to run the whole workflow, in alternative select the rule you want to run.
       ''')
  parser.add_argument("-n", "--dry-run", action="store_true", 
    help = '''
       "dry run" execution, i.e. snakemake will print out what it would do, but not actually do it.
       ''')
  parser.add_argument("-E", "--extraargs", type = str, 
    help = '''
       extra snakamake arguments. Must be provided within quotes and begin with a whitespace.
       For example, to have a dryrun: -E " -n"
       ''')
       
  # get arguments
  args      = parser.parse_args()

  # select snakefile corresponding to workflow
  snakefile = os.path.join(sc_dir, "rules.smk")
    
  # set default rule to all
  if args.rule == "":
    rule    = "all"
  else:
    rule = args.rule  
  
  extraargs = ''
  if args.extraargs:
    extraargs = args.extraargs
  if args.dry_run:
    extraargs = extraargs + " -np"
  
  # print(sc_dir, args.configfile, snakefile, rule)   
  # call bsub thing
  run_scprocess(sc_dir, args.configfile, snakefile, rule, extraargs)

